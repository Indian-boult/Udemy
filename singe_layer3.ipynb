{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import shadow_useragent\n",
    "import copy, sys, os, re, random\n",
    "import glob\n",
    "import threading\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError\n",
    "from fake_useragent import UserAgent\n",
    "from urllib2 import urlopen, Request\n",
    "from urllib2 import HTTPError\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_html(html):\n",
    "    soup = bs.BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    private = soup.find('div', attrs = {'class':'col-sm-4 col-sm-push-4 col-xs-8 col-xs-push-2'})\n",
    "    private_2 = soup.find('i', attrs = {'class': 'udi udi-warning'})\n",
    "    \n",
    "    enrollment = soup.find('div', attrs = {'data-purpose' : 'enrollment'})\n",
    "    last_update = soup.find('div', attrs = {'class': 'last-update-date'})\n",
    "    language = soup.find('div', attrs = {'class': 'clp-lead__locale'})\n",
    "    description = soup.find('div', attrs = {'class' : 'clp-lead__headline'})\n",
    "    categories = [cat.text.strip(\"\\n\") for cat in soup.find_all('a', attrs = {'class': 'topic-menu__link'})]\n",
    "    \n",
    "#     warning = soup.find('span', attrs = {'data-purpose': 'course-no-longer-published-warning' })\n",
    "    \n",
    "    if (private is not None) or (private_2 is not None):\n",
    "        print \"private\"\n",
    "        return [\"None\"] * 8\n",
    "    \n",
    "    try:\n",
    "        cat = [\" \", \" \", \" \"]\n",
    "        for idx in range(len(categories)):\n",
    "            cat[idx] = categories[idx]\n",
    "            \n",
    "        print cat[0]\n",
    "        print cat[1]\n",
    "        print cat[2]\n",
    "        print enrollment.text.strip(\" \\n\").split(\" \")[0]\n",
    "        print language.text.strip(\"\\n\")\n",
    "        print description.text.strip(\"\\n\")\n",
    "        print last_update.text.strip(\"\\n\").split(\" \")[-1]\n",
    "        print\n",
    "    \n",
    "        data = [cat[0], cat[1], cat[2], enrollment.text.strip(\" \\n\").split(\" \")[0], \n",
    "                language.text.strip(\"\\n\"), description.text.strip(\"\\n\"), last_update.text.strip(\"\\n\").split(\" \")[-1]]\n",
    "        return data\n",
    "    except IndexError:\n",
    "        raise\n",
    "        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = { 'http' : 'http://144.202.65.136:1440',\n",
    "            'http' : 'http://144.202.65.136:1441',\n",
    "            'http' : 'http://144.202.65.136:1442',\n",
    "            'http' : 'http://144.202.65.136:1443',\n",
    "            'http' : 'http://144.202.65.136:1444'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "private\n"
     ]
    }
   ],
   "source": [
    "ua = shadow_useragent.ShadowUserAgent()\n",
    "agent = ua.random\n",
    "headers = {'user-agent': agent}\n",
    "try:\n",
    "    req_url = 'http://www.udemy.com/course/the-best-aws-solutions-architect-associate/'\n",
    "    req_url2 = 'https://www.udemy.com/course/icerik-pazarlama-egitimi/'\n",
    "#     req_url2 = 'https://www.udemy.com/course/corso-web-marketing/'\n",
    "#     req_url2 = 'https://www.udemy.com/course/making-wealth-from-real-estate/'\n",
    "    req = requests.get(url=req_url2, proxies=proxies, headers=headers, timeout=5.0) \n",
    "    html = req.content\n",
    "    print req.status_code\n",
    "    if (req.url != req_url2):\n",
    "        print \"Does not exist\"\n",
    "    else:\n",
    "        df_single = scrape_html(html)\n",
    "except (ConnectTimeout, HTTPError, ReadTimeout, Timeout, ConnectionError, ):\n",
    "    print \"Too Long\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = robotparser.RobotFileParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.set_url(\"http://www.udemy.com/robots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named robotparser",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bba212bce97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobotparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named robotparser"
     ]
    }
   ],
   "source": [
    "import urllib.robotparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "RobotFileParser instance has no attribute 'crawl_delay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9bcde9423c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_delay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: RobotFileParser instance has no attribute 'crawl_delay'"
     ]
    }
   ],
   "source": [
    "rrate = rp.crawl_delay(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
